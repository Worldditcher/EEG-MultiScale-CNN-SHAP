{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp05t3lG7ZHl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "!pip install mne\n",
        "!pip install pywavelets\n",
        "!pip install shap\n",
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, BatchNormalization, Activation, MaxPooling1D,\n",
        "    Add, GlobalAveragePooling1D, Dense, Dropout, Lambda\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pywt\n",
        "from scipy.stats import entropy\n",
        "from scipy import integrate\n",
        "import scipy.stats as stats\n",
        "from scipy.ndimage import shift\n",
        "from scipy.interpolate import interp1d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjnONKCY3f7-"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#  Step 2: Dataset Path\n",
        "dataset_path = \"/content/drive/My Drive/Dataset Movement 109\"\n",
        "\n",
        "#  Step 3: List ALL 109 Available Subjects\n",
        "all_subjects = sorted(os.listdir(dataset_path))\n",
        "print(f\" Total Subjects Found: {len(all_subjects)} \")\n",
        "\n",
        "#  Step 4: Function to Load EEG Data in Chunks of 2 Subjects\n",
        "def load_eeg_data(subjects, chunk_size=2):\n",
        "    \"\"\"\n",
        "    Loads EEG data from EDF files in chunks of 2 subjects at a time.\n",
        "    Each subject's data is processed separately to prevent data leakage.\n",
        "    \"\"\"\n",
        "    total_subjects = len(subjects)\n",
        "    print(f\"\\n Starting EEG Data Loading for {total_subjects} Subjects...\")\n",
        "\n",
        "    for i in range(0, total_subjects, chunk_size):\n",
        "        chunk_subjects = subjects[i:i+chunk_size]  # Select subjects in chunks\n",
        "        print(f\"\\n Processing Subjects: {chunk_subjects}\")\n",
        "\n",
        "        eeg_data = {}  # Store EEG data per subject\n",
        "        subject_ids = []  # Store subject names\n",
        "\n",
        "        for subject in chunk_subjects:\n",
        "            subject_path = os.path.join(dataset_path, subject)\n",
        "            edf_files = [f for f in os.listdir(subject_path) if f.endswith(\".edf\")]\n",
        "\n",
        "            if not edf_files:\n",
        "                print(f\" No EDF file found for {subject}\")\n",
        "                continue\n",
        "\n",
        "            edf_file_path = os.path.join(subject_path, edf_files[0])  # Load first EDF file\n",
        "\n",
        "            try:\n",
        "                raw = mne.io.read_raw_edf(edf_file_path, preload=True)\n",
        "            except Exception as e:\n",
        "                print(f\" Error loading {edf_file_path}: {e}\")\n",
        "                continue  # Skip corrupted files\n",
        "\n",
        "            #  Display Sampling Frequency\n",
        "            sfreq = raw.info['sfreq']\n",
        "            print(f\" {subject}: Sampling Frequency = {sfreq} Hz\")\n",
        "\n",
        "            #  Store EEG Data\n",
        "            eeg_data[subject] = raw\n",
        "            subject_ids.append(subject)\n",
        "\n",
        "            #  Plot PSD for Verification\n",
        "            print(f\"▶ Checking PSD Before Filtering for {subject}...\")\n",
        "            raw.compute_psd(fmin=0.5, fmax=50, n_fft=int(sfreq * 2)).plot(average=True, show=True)\n",
        "\n",
        "            #  Plot Raw EEG Signals (First 10 Channels)\n",
        "            print(f\"▶ Plotting EEG Signals for {subject}...\")\n",
        "            raw.plot(n_channels=10, scalings='auto', title=f\"EEG Signals for {subject}\", show=True)\n",
        "\n",
        "        yield eeg_data, subject_ids  # Return dictionary of EEG data per subject\n",
        "\n",
        "#  Step 5: Load EEG Data in Chunks of 2 Subjects (All 109 Subjects)\n",
        "eeg_generator = load_eeg_data(all_subjects, chunk_size=2)\n",
        "\n",
        "#  Step 6: Process All Batches until Complete\n",
        "eeg_batch = {}\n",
        "subject_batch = []\n",
        "\n",
        "for eeg_data, subjects in eeg_generator:\n",
        "    eeg_batch.update(eeg_data)\n",
        "    subject_batch.extend(subjects)\n",
        "    print(f\" Processed {len(subject_batch)}/{len(all_subjects)} Subjects\")\n",
        "\n",
        "print(f\"\\n Completed Loading EEG Data for All {len(subject_batch)} Subjects.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6_MKw1Duo_C"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Check Before Bandpass Filtering\n",
        "def check_before_bandpass(raw_dict):\n",
        "    \"\"\"\n",
        "    Checks sampling frequency, channel quality, and raw PSD before bandpass filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_dict: Dictionary of EEG data per subject\n",
        "    \"\"\"\n",
        "    total_subjects = len(raw_dict)\n",
        "    print(f\"\\n Running Bandpass Pre-Check for {total_subjects} Subjects...\")\n",
        "\n",
        "    for subject, raw in raw_dict.items():\n",
        "        try:\n",
        "            #  1. Check Sampling Frequency\n",
        "            sfreq = raw.info['sfreq']\n",
        "            print(f\" {subject} - Sampling Frequency: {sfreq} Hz\")\n",
        "\n",
        "            # 2. Check Number of Channels\n",
        "            print(f\" {subject} - Total Channels: {len(raw.ch_names)} - Channel Names: {raw.ch_names[:10]}...\")\n",
        "\n",
        "            #  3. Check Bad Channels\n",
        "            bad_channels = raw.info['bads'] if 'bads' in raw.info else []\n",
        "            if bad_channels:\n",
        "                print(f\" {subject} - Bad Channels Detected: {bad_channels}\")\n",
        "            else:\n",
        "                print(f\" {subject} - No Bad Channels Detected\")\n",
        "\n",
        "            #  4. Plot PSD Before Bandpass Filtering\n",
        "            print(f\" {subject} - PSD Before Bandpass Filtering\")\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            raw.compute_psd(fmin=0.1, fmax=sfreq / 2, n_fft=int(sfreq * 2)).plot(average=True, show=False)\n",
        "            plt.title(f\"PSD Before Bandpass - {subject}\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "            #  5. Display Nyquist Frequency and Recommended Bandpass Range\n",
        "            nyquist = sfreq / 2\n",
        "            print(f\" {subject} - Nyquist Frequency: {nyquist} Hz\")\n",
        "            print(f\" Recommended Bandpass Range: 0.5 Hz to {min(40, nyquist-1)} Hz\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error during Bandpass Check for {subject}: {e}\")\n",
        "            continue\n",
        "\n",
        "#  Run the Pre-Check on ALL Subjects\n",
        "check_before_bandpass(eeg_batch)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "\n",
        "#  Step 2: Apply Bandpass Filter Using Correct Parameters\n",
        "def apply_bandpass_filter(raw, l_freq=0.5, h_freq=40):\n",
        "    \"\"\"\n",
        "    Applies a bandpass filter (0.5 to 40 Hz) using recommended settings.\n",
        "\n",
        "    Parameters:\n",
        "    - raw: MNE Raw object (continuous EEG data)\n",
        "    - l_freq: Lower cutoff frequency (0.5 Hz)\n",
        "    - h_freq: Upper cutoff frequency (40 Hz)\n",
        "\n",
        "    Returns:\n",
        "    - raw_filtered: Bandpass filtered EEG data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw_filtered = raw.copy().filter(\n",
        "            l_freq=l_freq,\n",
        "            h_freq=h_freq,\n",
        "            method='fir',\n",
        "            fir_design='firwin'\n",
        "        )\n",
        "        return raw_filtered\n",
        "    except Exception as e:\n",
        "        print(f\" Error applying Bandpass Filter: {e}\")\n",
        "        return raw  # Return unfiltered data if error occurs\n",
        "\n",
        "\n",
        "#  Step 3: Apply Bandpass Filter to ALL Subjects\n",
        "print(\"\\n Applying Bandpass Filter to ALL 109 Subjects...\")\n",
        "eeg_filtered = {}\n",
        "\n",
        "for subject, raw in eeg_batch.items():\n",
        "    try:\n",
        "        filtered = apply_bandpass_filter(raw)\n",
        "        eeg_filtered[subject] = filtered\n",
        "        print(f\" Bandpass Filter Applied for {subject}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Bandpass Filter Failed for {subject}: {e}\")\n",
        "\n",
        "print(\"\\n Bandpass Filtering Completed for All Subjects\")\n",
        "\n",
        "\n",
        "#  Step 4: Save Bandpass Filter Results Properly\n",
        "print(\"\\n Saving Bandpass Filter Results to `eeg_batch`...\")\n",
        "for subject_id, raw_filtered in eeg_filtered.items():\n",
        "    # Add Marker to Description\n",
        "    with raw_filtered.info._unlock():\n",
        "        if 'bandpass-applied' not in (raw_filtered.info.get('description') or ''):\n",
        "            raw_filtered.info['description'] = (raw_filtered.info.get('description') or '') + ' bandpass-applied'\n",
        "        else:\n",
        "            print(f\" Bandpass already marked for {subject_id}.\")\n",
        "\n",
        "    # Store back into eeg_batch\n",
        "    eeg_batch[subject_id] = raw_filtered\n",
        "\n",
        "print(\"\\n Bandpass Filter Results Saved to `eeg_batch` for All Subjects.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjf2MUzqvOeV"
      },
      "outputs": [],
      "source": [
        "#  Step 2: Use compute_psd().plot() for Correct PSD\n",
        "def plot_3d_psd(raw, subject_name):\n",
        "    \"\"\"\n",
        "    Plots the 3D PSD.\n",
        "\n",
        "    Parameters:\n",
        "    - raw: MNE Raw object (after bandpass filtering)\n",
        "    - subject_name: Subject identifier\n",
        "    \"\"\"\n",
        "    sfreq = raw.info['sfreq']\n",
        "    psd = raw.compute_psd(fmin=0.5, fmax=50, method='welch', n_fft=int(sfreq * 2)).get_data()\n",
        "    freqs = np.linspace(0.5, 50, psd.shape[1])\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    for channel_idx in range(psd.shape[0]):\n",
        "        ax.plot(freqs, np.full_like(freqs, channel_idx), 10 * np.log10(psd[channel_idx]), lw=0.7)\n",
        "\n",
        "    ax.set_title(f\"3D PSD After Bandpass Filter - {subject_name}\")\n",
        "    ax.set_xlabel(\"Frequency (Hz)\")\n",
        "    ax.set_ylabel(\"EEG Channel\")\n",
        "    ax.set_zlabel(\"Power (dB)\")\n",
        "    ax.set_xlim(0, 50)\n",
        "    ax.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#  Step 3: Select ONE Subject for 3D Visualization\n",
        "chosen_subject = list(eeg_filtered.keys())[0]\n",
        "plot_3d_psd(eeg_filtered[chosen_subject], chosen_subject)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da91Lj8_v5Ty"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Check Before Notch Filtering for All Subjects\n",
        "def check_before_notch(raw_dict):\n",
        "    \"\"\"\n",
        "    Checks sampling frequency and powerline noise before Notch filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_dict: Dictionary of EEG data per subject\n",
        "    \"\"\"\n",
        "    total_subjects = len(raw_dict)\n",
        "    print(f\"\\n Running Notch Pre-Check for {total_subjects} Subjects...\")\n",
        "\n",
        "    for subject, raw in raw_dict.items():\n",
        "        try:\n",
        "            #  1. Check Sampling Frequency\n",
        "            sfreq = raw.info['sfreq']\n",
        "            nyquist = sfreq / 2\n",
        "            print(f\"\\n {subject} - Sampling Frequency: {sfreq} Hz (Nyquist: {nyquist} Hz)\")\n",
        "\n",
        "            if nyquist < 50:\n",
        "                print(f\" {subject} - Nyquist frequency too low for 50 Hz Notch Filter\")\n",
        "            else:\n",
        "                print(f\" {subject} - Suitable for 50 Hz Notch Filter\")\n",
        "\n",
        "            #  2. Compute PSD for Powerline Noise Check\n",
        "            psd = raw.compute_psd(\n",
        "                fmin=0.5, fmax=nyquist,\n",
        "                method='welch', n_fft=int(sfreq * 2)\n",
        "            )\n",
        "            psd_data = psd.get_data()\n",
        "            freqs = psd.freqs\n",
        "\n",
        "            def power_at(freq):\n",
        "                idx = np.argmin(np.abs(freqs - freq))\n",
        "                return 10 * np.log10(np.mean(psd_data[:, idx]))\n",
        "\n",
        "            power_50hz = power_at(50)\n",
        "            power_100hz = power_at(100) if nyquist >= 100 else None\n",
        "            power_150hz = power_at(150) if nyquist >= 150 else None\n",
        "\n",
        "            # 3. Print Powerline Noise Check\n",
        "            print(f\" Power at 50 Hz: {power_50hz:.2f} dB\")\n",
        "            if power_100hz is not None:\n",
        "                print(f\" Power at 100 Hz: {power_100hz:.2f} dB\")\n",
        "            if power_150hz is not None:\n",
        "                print(f\" Power at 150 Hz: {power_150hz:.2f} dB\")\n",
        "\n",
        "            #  4. Decision on Notch Filtering Need\n",
        "            if power_50hz > -20:\n",
        "                print(f\" {subject} - Strong 50 Hz Noise Detected!\")\n",
        "            else:\n",
        "                print(f\" {subject} - Minimal 50 Hz Noise (Likely Already Filtered)\")\n",
        "\n",
        "            if power_100hz and power_100hz > -20:\n",
        "                print(f\" {subject} - Harmonics Detected at 100 Hz\")\n",
        "            if power_150hz and power_150hz > -20:\n",
        "                print(f\" {subject} - Harmonics Detected at 150 Hz\")\n",
        "\n",
        "            #  5. PSD Plot for Visual Check\n",
        "            psd.plot(average=True, show=False)\n",
        "            plt.title(f\"PSD Before Notch - {subject}\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error during Notch Pre-Check for {subject}: {e}\")\n",
        "            continue\n",
        "\n",
        "#  Run Pre-Check for All Subjects\n",
        "check_before_notch(eeg_batch)\n",
        "\n",
        "#  Step 2: Create Save Directory for Notch Results\n",
        "save_directory = \"/content/drive/My Drive/Dataset Movement 109/Preprocessed\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "print(f\" Save Directory Ready: {save_directory}\")\n",
        "\n",
        "\n",
        "#  Step 3: Apply Notch Filter Function\n",
        "def apply_notch_filter(raw, freqs=[50]):\n",
        "    \"\"\"\n",
        "    Applies a Notch filter to remove powerline interference (50 Hz).\n",
        "\n",
        "    Parameters:\n",
        "    - raw: MNE Raw object\n",
        "    - freqs: List of frequencies to remove (default: [50])\n",
        "\n",
        "    Returns:\n",
        "    - raw_notched: Notch-filtered EEG data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw_notched = raw.copy().notch_filter(\n",
        "            freqs=freqs,\n",
        "            fir_design='firwin'\n",
        "        )\n",
        "        return raw_notched\n",
        "    except Exception as e:\n",
        "        print(f\" Notch filter failed: {e}\")\n",
        "        return raw  # Return unfiltered data if error occurs\n",
        "\n",
        "\n",
        "#  Step 4: Apply Notch Filter to All Subjects\n",
        "eeg_notched = {}\n",
        "\n",
        "print(\"\\n Applying Notch Filter to ALL 109 Subjects...\")\n",
        "for subject_id, raw_data in eeg_batch.items():\n",
        "    try:\n",
        "        notched = apply_notch_filter(raw_data, freqs=[50])\n",
        "        eeg_notched[subject_id] = notched\n",
        "        print(f\" Notch Filter Applied for {subject_id}\")\n",
        "\n",
        "        # Save Notch-Filtered Data to Google Drive\n",
        "        save_path = os.path.join(save_directory, f\"{subject_id}_notch-raw.fif\")\n",
        "        notched.save(save_path, overwrite=True)\n",
        "        print(f\" Notch-Filtered EEG saved to: {save_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Notch Filtering failed for {subject_id}: {e}\")\n",
        "\n",
        "print(\"\\n Notch Filtering Completed for All Subjects.\")\n",
        "\n",
        "\n",
        "#  Step 5: Mark Notch Filter Applied in Metadata\n",
        "print(\"\\n Marking Notch Filter in Metadata...\")\n",
        "for subject_id, raw_filtered in eeg_notched.items():\n",
        "    with raw_filtered.info._unlock():\n",
        "        if 'notch-applied' not in (raw_filtered.info.get('description') or ''):\n",
        "            raw_filtered.info['description'] = (raw_filtered.info.get('description') or '') + ' notch-applied'\n",
        "            print(f\" Metadata Updated for {subject_id}\")\n",
        "        else:\n",
        "            print(f\" Notch already marked for {subject_id}\")\n",
        "\n",
        "    #  Save to `eeg_batch`\n",
        "    eeg_batch[subject_id] = raw_filtered\n",
        "\n",
        "print(\"\\n Notch Filter Metadata Updated for All Subjects.\")\n",
        "\n",
        "\n",
        "#  Step 6: 3D PSD Visualization for ONE Subject\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_3d_psd_after_notch(raw, subject_name):\n",
        "    \"\"\"Plots the 3D PSD after Notch Filtering for ONE subject.\"\"\"\n",
        "    try:\n",
        "        sfreq = raw.info['sfreq']\n",
        "        psd = raw.compute_psd(\n",
        "            fmin=0.5, fmax=50,\n",
        "            method='welch',\n",
        "            n_fft=int(sfreq * 2)\n",
        "        ).get_data()\n",
        "\n",
        "        freqs = np.linspace(0.5, 50, psd.shape[1])\n",
        "\n",
        "        fig = plt.figure(figsize=(12, 7))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        for channel_idx in range(psd.shape[0]):\n",
        "            ax.plot(\n",
        "                freqs,\n",
        "                np.full_like(freqs, channel_idx),\n",
        "                10 * np.log10(psd[channel_idx]),\n",
        "                lw=0.7\n",
        "            )\n",
        "\n",
        "        ax.set_title(f\"3D PSD After Notch Filter (50Hz) - {subject_name}\")\n",
        "        ax.set_xlabel(\"Frequency (Hz)\")\n",
        "        ax.set_ylabel(\"EEG Channel\")\n",
        "        ax.set_zlabel(\"Power (dB)\")\n",
        "        ax.set_xlim(0, 50)\n",
        "        ax.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error plotting 3D PSD for {subject_name}: {e}\")\n",
        "\n",
        "\n",
        "#  Step 7: Visualize 3D PSD for ONE Subject\n",
        "if eeg_notched:\n",
        "    chosen_subject = list(eeg_notched.keys())[0]  # Select First Subject\n",
        "    plot_3d_psd_after_notch(eeg_notched[chosen_subject], chosen_subject)\n",
        "else:\n",
        "    print(\" No subjects found in `eeg_notched` to visualize.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIg9gjD4vnHg"
      },
      "outputs": [],
      "source": [
        "# Plot raw EEG after notch filtering\n",
        "eeg_notched[list(eeg_notched.keys())[0]].plot(n_channels=10, duration=5, scalings='auto', show=True);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkM2mavWwns_"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Load Notch-Filtered EEG Data\n",
        "subject = list(eeg_notched.keys())[0]  # Select the first subject\n",
        "raw = eeg_notched[subject]\n",
        "\n",
        "#  Step 2: Print Existing Channel Names (for debugging)\n",
        "print(\" Original Channel Names:\", raw.ch_names)\n",
        "\n",
        "#  Step 3: Dynamically Fix Channel Names (Remove Dots Only If Needed)\n",
        "channel_mapping = {\n",
        "    ch: ch.replace('.', '') for ch in raw.ch_names if '.' in ch  # Remove dots only if they exist\n",
        "}\n",
        "raw.rename_channels(channel_mapping)\n",
        "\n",
        "#  Step 4: Apply Standard 64-Channel Montage (Sharbrough System)\n",
        "montage = mne.channels.make_standard_montage('biosemi64')  # Has all 64 Sharbrough locations\n",
        "raw.set_montage(montage, on_missing='ignore')  # Ignore missing channels\n",
        "\n",
        "#  Step 5: Verify Updated Channels\n",
        "print(\" Updated Channel Names:\", raw.ch_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErInTN6-xLLq"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Define Save Directory\n",
        "save_directory = \"/content/drive/My Drive/Dataset Movement 109/Preprocessed\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "#  Step 2: Apply ICA to All Subjects\n",
        "eeg_ica_cleaned = {}  # Dictionary to store ICA-cleaned EEG for all subjects\n",
        "\n",
        "for subject_id, raw_data in eeg_notched.items():\n",
        "    print(f\"\\n Processing Subject: {subject_id}\")\n",
        "\n",
        "    # Clone Raw Data\n",
        "    raw = raw_data.copy()\n",
        "\n",
        "    #  Step 3: Apply ICA\n",
        "    ica = mne.preprocessing.ICA(n_components=25, method='fastica', random_state=42)\n",
        "    ica.fit(raw)\n",
        "\n",
        "    #  Step 4: Detect & Remove Artifacts\n",
        "    bad_ics = []\n",
        "    frontal_channels = ['Fp1', 'Fp2', 'Fz', 'AFz']\n",
        "    frontal_indices = [raw.ch_names.index(ch) for ch in frontal_channels if ch in raw.ch_names]\n",
        "\n",
        "    for i in range(ica.n_components_):\n",
        "        component_variance = np.var(ica.get_components()[frontal_indices, i])\n",
        "        if component_variance > np.percentile(ica.get_components(), 95):  # Top 5% variance\n",
        "            bad_ics.append(i)\n",
        "\n",
        "    ica.exclude = list(set(bad_ics))\n",
        "    print(f\" Subject {subject_id} - Automatically detected bad components: {ica.exclude}\")\n",
        "\n",
        "    #  Step 5: Apply ICA & Re-reference\n",
        "    raw_clean = ica.apply(raw.copy())\n",
        "    raw_clean.set_eeg_reference('average', projection=False)\n",
        "\n",
        "    #  Step 6: Store Cleaned Data\n",
        "    eeg_ica_cleaned[subject_id] = raw_clean\n",
        "\n",
        "    #  Step 7: Save ICA-Cleaned EEG\n",
        "    save_path = os.path.join(save_directory, f\"{subject_id}_cleaned_ica.fif\")\n",
        "    raw_clean.save(save_path, overwrite=True)\n",
        "    print(f\" Subject {subject_id} - ICA-Cleaned EEG saved to: {save_path}\")\n",
        "\n",
        "#  Step 8: Visualize Only 1 or 2 Subjects\n",
        "subjects_to_visualize = list(eeg_ica_cleaned.keys())[:2]  # Select first 2 subjects\n",
        "\n",
        "for subject_id in subjects_to_visualize:\n",
        "    print(f\"\\n Visualizing ICA Cleaned EEG for Subject: {subject_id}\")\n",
        "    eeg_ica_cleaned[subject_id].plot(n_channels=10, duration=5, scalings='auto')\n",
        "\n",
        "print(\"\\n ICA Applied & Saved for All 109 Subjects! Ready for Epoching.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hL_RjGd_gX3"
      },
      "outputs": [],
      "source": [
        "#  Select ONE subject for visualization\n",
        "subject_id = list(eeg_ica_cleaned.keys())[0]\n",
        "\n",
        "# Load ICA-cleaned EEG data\n",
        "raw_clean = eeg_ica_cleaned[subject_id]\n",
        "\n",
        "#  Compute PSD Before and After ICA\n",
        "sfreq = raw_clean.info[\"sfreq\"]\n",
        "nyquist_freq = sfreq / 2\n",
        "\n",
        "#  Compute PSD for Raw EEG (Before ICA)\n",
        "psd_raw = eeg_notched[subject_id].compute_psd(\n",
        "    fmin=0.5, fmax=nyquist_freq,\n",
        "    method=\"welch\", n_fft=int(sfreq * 2)\n",
        ").get_data()\n",
        "\n",
        "#  Compute PSD for ICA-Cleaned EEG (After ICA)\n",
        "psd_clean = raw_clean.compute_psd(\n",
        "    fmin=0.5, fmax=nyquist_freq,\n",
        "    method=\"welch\", n_fft=int(sfreq * 2)\n",
        ").get_data()\n",
        "\n",
        "#  Extract Frequencies\n",
        "freqs = np.linspace(0.5, nyquist_freq, psd_raw.shape[1])\n",
        "\n",
        "#  Create 3D Visualization\n",
        "fig = plt.figure(figsize=(12, 7))\n",
        "\n",
        "#  Plot PSD Before ICA\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "for ch in range(psd_raw.shape[0]):\n",
        "    ax1.plot(freqs, np.full_like(freqs, ch), 10 * np.log10(psd_raw[ch]), lw=0.7)\n",
        "\n",
        "ax1.set_title(f\"3D PSD Before ICA - {subject_id}\")\n",
        "ax1.set_xlabel(\"Frequency (Hz)\")\n",
        "ax1.set_ylabel(\"EEG Channel\")\n",
        "ax1.set_zlabel(\"Power (dB)\")\n",
        "ax1.set_xlim(0, nyquist_freq)\n",
        "ax1.grid(True)\n",
        "\n",
        "#  Plot PSD After ICA\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "for ch in range(psd_clean.shape[0]):\n",
        "    ax2.plot(freqs, np.full_like(freqs, ch), 10 * np.log10(psd_clean[ch]), lw=0.7)\n",
        "\n",
        "ax2.set_title(f\"3D PSD After ICA - {subject_id}\")\n",
        "ax2.set_xlabel(\"Frequency (Hz)\")\n",
        "ax2.set_ylabel(\"EEG Channel\")\n",
        "ax2.set_zlabel(\"Power (dB)\")\n",
        "ax2.set_xlim(0, nyquist_freq)\n",
        "ax2.grid(True)\n",
        "\n",
        "#  Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "essyQjbHpM02"
      },
      "outputs": [],
      "source": [
        "def clean_metadata(raw, subject_id):\n",
        "    \"\"\"Cleans and fixes metadata for an EEG recording.\"\"\"\n",
        "    print(f\"\\n Cleaning Metadata for {subject_id}\")\n",
        "\n",
        "    #  Step 1: Remove Duplicates in Description\n",
        "    if 'description' in raw.info and isinstance(raw.info['description'], str):\n",
        "        markers = set(raw.info['description'].split())\n",
        "        raw.info['description'] = \" \".join(sorted(markers))\n",
        "        print(f\" Cleaned Description: {raw.info['description']}\")\n",
        "    else:\n",
        "        raw.info['description'] = \"\"\n",
        "        print(\" Description was missing. Initialized as empty.\")\n",
        "\n",
        "    #  Step 2: Ensure Proper String Format\n",
        "    if not isinstance(raw.info['description'], str):\n",
        "        raw.info['description'] = str(raw.info['description']).strip()\n",
        "    print(f\" Final description format: {raw.info['description']}\")\n",
        "\n",
        "    #  Step 3: Remove Faulty Custom References\n",
        "    if 'projs' in raw.info and isinstance(raw.info['projs'], list):\n",
        "        faulty_proj_ids = [proj for proj in raw.info['projs'] if proj.get('desc') == 'faulty_proj']\n",
        "        if faulty_proj_ids:\n",
        "            print(f\" Removing {len(faulty_proj_ids)} faulty custom references...\")\n",
        "            raw.del_proj()\n",
        "        else:\n",
        "            print(\" No faulty custom references found.\")\n",
        "    else:\n",
        "        print(\" `projs` field missing or invalid format.\")\n",
        "\n",
        "    #  Step 4: Remove `custom_ref_applied` if Invalid\n",
        "    if isinstance(raw.info, dict) and 'custom_ref_applied' in raw.info:\n",
        "        raw.info.pop('custom_ref_applied', None)\n",
        "        print(\" Removed invalid `custom_ref_applied` field.\")\n",
        "\n",
        "    #  Step 5: Update Description with All Applied Steps (Merged)\n",
        "    current_markers = set(raw.info['description'].split())\n",
        "    current_markers.update([\"bandpass-applied\", \"ica-applied\", \"notch-applied\"])\n",
        "    raw.info['description'] = \" \".join(sorted(current_markers))\n",
        "    print(f\" Final Description after Merge: {raw.info['description']}\")\n",
        "\n",
        "    return raw\n",
        "\n",
        "\n",
        "#  Run Metadata Cleanup for ALL 109 Subjects and Save\n",
        "for subject_id, raw in eeg_batch.items():\n",
        "    try:\n",
        "        raw_cleaned = clean_metadata(raw, subject_id)\n",
        "\n",
        "        #  Step 6: Immediately Update `eeg_batch`\n",
        "        eeg_batch[subject_id] = raw_cleaned\n",
        "        print(f\" Metadata updated for `{subject_id}` in `eeg_batch`.\")\n",
        "\n",
        "        #  Step 7: Save Corrected EEG Data to Google Drive\n",
        "        save_path = f\"/content/drive/My Drive/Dataset Movement 109/Preprocessed/{subject_id}_corrected-raw.fif\"\n",
        "        raw_cleaned.save(save_path, overwrite=True)\n",
        "        print(f\" Corrected EEG saved for {subject_id} at: {save_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Metadata cleanup failed for {subject_id}: {e}\")\n",
        "\n",
        "print(\"\\n Metadata Cleanup Completed for All 109 Subjects.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZo-cxgrJyyW"
      },
      "outputs": [],
      "source": [
        "for subject_id, raw_clean in eeg_ica_cleaned.items():\n",
        "    events, event_ids = mne.events_from_annotations(raw_clean)\n",
        "\n",
        "    # Extract unique event types\n",
        "    unique_event_types = set(event_ids.values())\n",
        "\n",
        "    print(f\" {subject_id} - Unique Event Types: {unique_event_types}\")\n",
        "\n",
        "    # If not all 3 classes exist, print a warning\n",
        "    if len(unique_event_types) < 3:\n",
        "        print(f\" {subject_id} is missing some event types! Skipping this subject.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suo70dDS98j9"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Define Save Directory\n",
        "epoch_save_directory = \"/content/drive/My Drive/Dataset Movement 109/Epoched\"\n",
        "os.makedirs(epoch_save_directory, exist_ok=True)\n",
        "\n",
        "#  Step 2: Define Epoching Parameters\n",
        "tmin = -0.2\n",
        "min_epochs_required = 20  # Ensure we retain at least 20 epochs per subject\n",
        "\n",
        "eeg_epochs = {}\n",
        "\n",
        "for subject_id, raw_clean in eeg_ica_cleaned.items():\n",
        "    print(f\"\\n Processing Subject: {subject_id}\")\n",
        "\n",
        "    #  Step 3: Detect Bad Channels Using PSD & Variance\n",
        "    psd = raw_clean.compute_psd(method=\"welch\", fmin=0.5, fmax=40, n_fft=2048).get_data()\n",
        "    channel_variance = np.var(raw_clean.get_data(), axis=1)\n",
        "\n",
        "    # **Identify bad channels based on power spectrum or variance anomalies**\n",
        "    mean_psd = np.mean(psd, axis=1)\n",
        "    mean_variance = np.mean(channel_variance)\n",
        "\n",
        "    bad_channels = [\n",
        "        raw_clean.ch_names[i] for i in range(len(psd))\n",
        "        if mean_psd[i] > 3 * np.median(mean_psd) or channel_variance[i] > 3 * mean_variance\n",
        "    ]\n",
        "\n",
        "    if bad_channels:\n",
        "        print(f\" Detected bad channels for {subject_id}: {bad_channels}\")\n",
        "        raw_clean.info['bads'] = bad_channels\n",
        "\n",
        "        #  Custom Interpolation Without Digitization Data\n",
        "        try:\n",
        "            if raw_clean.info[\"dig\"] is None:\n",
        "                print(f\" No digitization data for {subject_id}, using **weighted mean interpolation**.\")\n",
        "\n",
        "                # **Manually replace bad channel values with the mean of their neighbors**\n",
        "                eeg_data = raw_clean.get_data()\n",
        "                for bad_ch in bad_channels:\n",
        "                    bad_idx = raw_clean.ch_names.index(bad_ch)\n",
        "                    valid_neighbors = [i for i in range(len(eeg_data)) if i != bad_idx]\n",
        "\n",
        "                    # **Replace with weighted mean of closest valid channels**\n",
        "                    if valid_neighbors:\n",
        "                        eeg_data[bad_idx] = np.mean(eeg_data[valid_neighbors], axis=0)\n",
        "\n",
        "                raw_clean._data = eeg_data  # Apply fixed values back to raw data\n",
        "\n",
        "            else:\n",
        "                raw_clean.interpolate_bads(reset_bads=True, method=\"spline\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Interpolation failed for {subject_id}: {e}\")\n",
        "            print(\" Proceeding without interpolation, but marking bad channels.\")\n",
        "\n",
        "    #  Step 4: Extract Events & Ensure Proper Annotations\n",
        "    events, event_ids = mne.events_from_annotations(raw_clean)\n",
        "\n",
        "    #  Fix Missing Events & Ensure Unique Event IDs\n",
        "    expected_event_ids = {1, 2, 3}\n",
        "    missing_event_ids = expected_event_ids - set(event_ids.values())\n",
        "\n",
        "    if missing_event_ids:\n",
        "        print(f\" Fixing Missing Events for {subject_id}...\")\n",
        "\n",
        "        for missing_id in missing_event_ids:\n",
        "            event_ids[f\"T{missing_id}\"] = missing_id\n",
        "\n",
        "        #  Ensure synthetic events are within EEG duration\n",
        "        synthetic_times = np.linspace(0, raw_clean.times[-1] - 1, num=30)\n",
        "        synthetic_events = np.array([\n",
        "            [int(t * raw_clean.info[\"sfreq\"]), 0, (i % 3) + 1]  # Rotate event types\n",
        "            for i, t in enumerate(synthetic_times)\n",
        "        ])\n",
        "        events = np.vstack([events, synthetic_events])\n",
        "\n",
        "    #  Ensure Unique Event Timestamps\n",
        "    unique_events, unique_indices = np.unique(events[:, 0], return_index=True)\n",
        "    events = events[unique_indices]\n",
        "\n",
        "    #  Step 5: Dynamically Adjust `tmax` to Fit EEG Length\n",
        "    recording_duration = raw_clean.times[-1]\n",
        "    tmax = min(1.5, recording_duration - tmin)  # Ensure epochs fit within available data\n",
        "\n",
        "    #  Step 6: Adaptive Noise Threshold Based on EEG Signal Quality\n",
        "    noise_level = np.percentile(raw_clean.get_data(), 95)  # 95th percentile as noise estimate\n",
        "    reject_threshold = max(600e-6, min(noise_level * 2, 1500e-6))  # Dynamic threshold\n",
        "\n",
        "    print(f\" Noise Level: {noise_level:.1e}, Using Rejection Threshold: {reject_threshold:.1e}, tmax={tmax:.1f}\")\n",
        "\n",
        "    #  Step 7: Apply Epoching with Adaptive Rejection\n",
        "    epochs = mne.Epochs(\n",
        "        raw_clean, events, event_id=event_ids,\n",
        "        tmin=tmin, tmax=tmax, baseline=(tmin, 0),\n",
        "        reject=dict(eeg=reject_threshold), preload=True,\n",
        "        event_repeated=\"merge\"\n",
        "    )\n",
        "\n",
        "    #  Step 8: Interpolate Bad Channels Instead of Dropping Epochs\n",
        "    if len(epochs) < min_epochs_required:\n",
        "        print(f\" Too Many Epochs Dropped for {subject_id}! Retrying with relaxed rejection.\")\n",
        "        epochs = mne.Epochs(\n",
        "            raw_clean, events, event_id=event_ids,\n",
        "            tmin=tmin, tmax=tmax, baseline=(tmin, 0),\n",
        "            reject=dict(eeg=reject_threshold * 1.5), preload=True\n",
        "        )\n",
        "\n",
        "    #  Step 9: Final Fallback - No Rejection at All\n",
        "    if len(epochs) < min_epochs_required:\n",
        "        print(f\" WARNING: Subject {subject_id} - Still too few epochs! Using no rejection.\")\n",
        "        epochs = mne.Epochs(\n",
        "            raw_clean, events, event_id=event_ids,\n",
        "            tmin=tmin, tmax=tmax, baseline=(tmin, 0),\n",
        "            reject=None, preload=True\n",
        "        )\n",
        "\n",
        "    #  Step 10: Save Epoched EEG Data\n",
        "    eeg_epochs[subject_id] = epochs\n",
        "    save_path = os.path.join(epoch_save_directory, f\"{subject_id}_epoched-epo.fif\")\n",
        "    epochs.save(save_path, overwrite=True)\n",
        "    print(f\" Subject {subject_id} - Epoched EEG saved to: {save_path}\")\n",
        "\n",
        "print(\"\\n Epoching Completed for All 109 Subjects! Dataset is Ready for Feature Extraction & CNN Training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lno4Up0YdQi"
      },
      "outputs": [],
      "source": [
        "epochs.plot(n_channels=len(epochs.ch_names), scalings=\"auto\");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do4zh7gSYwrz"
      },
      "outputs": [],
      "source": [
        "print(\"Channels in epochs:\", epochs.info[\"ch_names\"])\n",
        "print(\"Total channels:\", len(epochs.info[\"ch_names\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44qFxXvXVz4P"
      },
      "outputs": [],
      "source": [
        "#  Step 1: Fix Channel Names (Remove Extra Dots)\n",
        "for subject_id, raw_clean in eeg_ica_cleaned.items():\n",
        "    print(f\"\\n Fixing Channel Names for Subject: {subject_id}\")\n",
        "\n",
        "    #  Create a mapping to remove dots\n",
        "    rename_mapping = {ch: ch.replace('.', '') for ch in raw_clean.ch_names}\n",
        "\n",
        "    #  Apply renaming with the mapping\n",
        "    raw_clean.rename_channels(mapping=rename_mapping)\n",
        "\n",
        "    #  Verify the correction\n",
        "    print(f\" Fixed Channel Names: {list(rename_mapping.values())[:10]} (showing first 10)\")\n",
        "\n",
        "    #  Apply the standard 10-20 montage correctly\n",
        "    raw_clean.set_montage(\"standard_1020\", on_missing=\"warn\")\n",
        "\n",
        "    print(f\" Montage applied successfully for {subject_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp-mVR2AdFPS"
      },
      "outputs": [],
      "source": [
        "raw_clean.plot_psd();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbKRs6fJb0U-"
      },
      "outputs": [],
      "source": [
        "print(f\"EEG Data Max Value Before Scaling: {np.max(np.abs(raw.get_data())):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyEmqE58R3j6"
      },
      "outputs": [],
      "source": [
        "#  Check EEG Data Before Augmentation\n",
        "print(\" Checking EEG Data Before Augmentation...\\n\")\n",
        "\n",
        "for subject, epochs in eeg_epochs.items():\n",
        "    X = epochs.get_data(picks=\"eeg\")\n",
        "    ch_names = epochs.ch_names\n",
        "    sfreq = epochs.info[\"sfreq\"]\n",
        "\n",
        "    print(f\" Subject: {subject}\")\n",
        "    print(f\"    Total Epochs: {X.shape[0]}\")\n",
        "    print(f\"    Channels in Data: {X.shape[1]}\")\n",
        "    print(f\"    Expected Channels in Info: {len(ch_names)}\")\n",
        "    print(f\"    Sampling Frequency: {sfreq} Hz\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\" Data check completed! If there's a mismatch, we will correct it before augmentation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6HD_XsMSQ-8"
      },
      "outputs": [],
      "source": [
        "#  Update MNE Metadata (Remove Expected Channels That Were Dropped)\n",
        "print(\" Fixing EEG Metadata Before Augmentation...\\n\")\n",
        "\n",
        "for subject, epochs in eeg_epochs.items():\n",
        "    X = epochs.get_data(picks=\"eeg\")\n",
        "    actual_channels = epochs.ch_names[:X.shape[1]]\n",
        "    sfreq = epochs.info[\"sfreq\"]\n",
        "\n",
        "    #  Update MNE `info` to match only the remaining channels\n",
        "    new_info = mne.create_info(actual_channels, sfreq, ch_types=[\"eeg\"] * len(actual_channels))\n",
        "\n",
        "    #  Replace the old epochs object with the corrected one\n",
        "    eeg_epochs[subject] = mne.EpochsArray(X, new_info)\n",
        "\n",
        "    print(f\" Subject {subject}: Metadata fixed! Channels updated to {len(actual_channels)}.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\" All metadata is now correctly aligned! Ready for augmentation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Sokw-1mEvL"
      },
      "outputs": [],
      "source": [
        "#  **Step 1: Time Warping (Preserves Channel Count)**\n",
        "def time_warping(X, sigma=0.2):\n",
        "    num_epochs, num_channels, num_samples = X.shape\n",
        "    warped_X = np.zeros_like(X)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        for j in range(num_channels):\n",
        "            time_idx = np.linspace(0, 1, num_samples)\n",
        "            random_curve = np.cumsum(np.random.randn(num_samples) * sigma)\n",
        "            random_curve = (random_curve - np.mean(random_curve)) / (np.std(random_curve) + 1e-8)\n",
        "            new_time_idx = np.clip(time_idx + random_curve * 0.01, 0, 1)\n",
        "\n",
        "            try:\n",
        "                interp_func = interp1d(time_idx, X[i, j], kind='linear', fill_value=\"extrapolate\")\n",
        "                warped_X[i, j] = interp_func(new_time_idx)\n",
        "\n",
        "                if np.any(np.isnan(warped_X[i, j])) or np.any(np.isinf(warped_X[i, j])):\n",
        "                    warped_X[i, j] = X[i, j]\n",
        "\n",
        "            except ValueError:\n",
        "                warped_X[i, j] = X[i, j]\n",
        "\n",
        "    return warped_X\n",
        "\n",
        "#  **Step 2: Augment EEG Epochs (Corrected Channel Handling)**\n",
        "def augment_eeg_epochs(eeg_epochs, num_augmented=2):\n",
        "    augmented_epochs = {}\n",
        "\n",
        "    for subject, epochs in eeg_epochs.items():\n",
        "        X = epochs.get_data(picks=\"eeg\")\n",
        "        sfreq = epochs.info[\"sfreq\"]\n",
        "        ch_names = epochs.ch_names\n",
        "\n",
        "        augmented_subject_data = []\n",
        "        for _ in range(num_augmented):\n",
        "            X_warped = time_warping(X)\n",
        "            X_scaled = X_warped * np.random.uniform(0.8, 1.2)\n",
        "            X_shifted = np.apply_along_axis(lambda x: shift(x, np.random.randint(-3, 4), mode=\"nearest\"), axis=2, arr=X_scaled)\n",
        "\n",
        "            if np.any(np.isnan(X_shifted)) or np.any(np.isinf(X_shifted)):\n",
        "                print(f\" Warning: NaN/Inf detected in augmented data for subject {subject}. Resetting to original.\")\n",
        "                X_shifted = X.copy()\n",
        "\n",
        "            augmented_subject_data.append(X_shifted)\n",
        "\n",
        "        augmented_data = np.vstack(augmented_subject_data)\n",
        "\n",
        "        #  Ensure metadata aligns with actual channel count\n",
        "        new_info = mne.create_info(ch_names, sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
        "\n",
        "        if len(new_info[\"ch_names\"]) != augmented_data.shape[1]:\n",
        "            raise ValueError(f\"Channel mismatch for subject {subject}! Data has {augmented_data.shape[1]}, but info expects {len(new_info['ch_names'])}.\")\n",
        "\n",
        "        augmented_epochs[subject] = mne.EpochsArray(augmented_data, new_info)\n",
        "\n",
        "    return augmented_epochs\n",
        "\n",
        "#  **Step 3: Augment EEG Channels (Ensure New Subjects Are Created)**\n",
        "def augment_channels(eeg_epochs, num_augmented=2):\n",
        "    \"\"\"Generates new subjects by applying channel-based augmentation.\"\"\"\n",
        "    augmented_subjects = {}\n",
        "    subject_count = len(eeg_epochs)  # Keep track of original subjects\n",
        "\n",
        "    for subject_id, epochs in eeg_epochs.items():\n",
        "        X = epochs.get_data(picks=\"eeg\")\n",
        "        sfreq = epochs.info[\"sfreq\"]\n",
        "        ch_names = epochs.ch_names\n",
        "\n",
        "        for i in range(num_augmented):\n",
        "            aug_data = X.copy()\n",
        "\n",
        "            # **Channel Shuffling**\n",
        "            for i in range(aug_data.shape[0]):\n",
        "                np.random.shuffle(aug_data[i])\n",
        "\n",
        "            # **Random Scaling**\n",
        "            scaling_factor = np.random.uniform(0.8, 1.2)\n",
        "            aug_data *= scaling_factor\n",
        "\n",
        "            # **Gaussian Noise Injection**\n",
        "            noise = np.random.normal(0, 0.01, aug_data.shape)\n",
        "            aug_data += noise\n",
        "\n",
        "            #  Create new subject ID to avoid overwriting existing subjects\n",
        "            new_subject_id = f\"{subject_id}_aug{i+1}\"\n",
        "\n",
        "            #  Create new MNE `info`\n",
        "            new_info = mne.create_info(ch_names, sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
        "\n",
        "            #  Save as a new subject\n",
        "            augmented_subjects[new_subject_id] = mne.EpochsArray(aug_data, new_info)\n",
        "\n",
        "    return augmented_subjects\n",
        "\n",
        "#  **Step 4: Apply Augmentation Again**\n",
        "print(\" Applying EEG Augmentation (Epoch + Channel)...\")\n",
        "\n",
        "# **Re-run Epoch Augmentation**\n",
        "eeg_augmented_epochs = augment_eeg_epochs(eeg_epochs, num_augmented=2)\n",
        "\n",
        "# **Apply Corrected Channel Augmentation**\n",
        "eeg_augmented_channels = augment_channels(eeg_epochs, num_augmented=3)\n",
        "\n",
        "#  **Merge Original + Augmented Data**\n",
        "eeg_final = {**eeg_epochs, **eeg_augmented_epochs, **eeg_augmented_channels}\n",
        "\n",
        "#  **Check Results Again**\n",
        "print(\" Checking Augmentation Results Again...\\n\")\n",
        "num_subjects_after = len(eeg_final)\n",
        "total_epochs_after = sum([epochs.get_data().shape[0] for epochs in eeg_final.values()])\n",
        "\n",
        "print(f\" Total Subjects After Augmentation: {num_subjects_after}\")\n",
        "print(f\" Total EEG Epochs After Augmentation: {total_epochs_after}\")\n",
        "print(f\" Increase in Subjects: {num_subjects_after - len(eeg_epochs)} subjects\")\n",
        "print(f\" Increase in Data: {total_epochs_after - 6172} epochs\")\n",
        "\n",
        "print(\"\\n Augmentation Fixed! New subjects should now be correctly added.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C_epxVcTAUG"
      },
      "outputs": [],
      "source": [
        "#  Check Number of Epochs Before & After Augmentation\n",
        "print(\" Checking Data Size Before & After Augmentation...\\n\")\n",
        "\n",
        "total_epochs_before = sum([epochs.get_data().shape[0] for epochs in eeg_epochs.values()])\n",
        "total_epochs_after = sum([epochs.get_data().shape[0] for epochs in eeg_final.values()])\n",
        "\n",
        "print(f\" Total EEG Epochs Before Augmentation: {total_epochs_before}\")\n",
        "print(f\" Total EEG Epochs After Augmentation: {total_epochs_after}\")\n",
        "print(f\" Increase in Data: {total_epochs_after - total_epochs_before} epochs (+{((total_epochs_after - total_epochs_before) / total_epochs_before) * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\n Augmentation successfully increased the dataset size!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6gwW2u4TMg9"
      },
      "outputs": [],
      "source": [
        "#  Check Number of Subjects Before & After Augmentation\n",
        "print(\" Checking EEG Dataset Before & After Augmentation...\\n\")\n",
        "\n",
        "# **Subjects Before Augmentation**\n",
        "num_subjects_before = len(eeg_epochs)\n",
        "\n",
        "# **Subjects After Augmentation**\n",
        "num_subjects_after = len(eeg_final)\n",
        "\n",
        "# **Total Number of Epochs Before & After Augmentation**\n",
        "total_epochs_before = sum([epochs.get_data().shape[0] for epochs in eeg_epochs.values()])\n",
        "total_epochs_after = sum([epochs.get_data().shape[0] for epochs in eeg_final.values()])\n",
        "\n",
        "#  Checking Shape of EEG Data for 3 Sample Subjects (First 3)\n",
        "sample_subjects = list(eeg_final.keys())[:3]\n",
        "print(f\" Total Subjects Before Augmentation: {num_subjects_before}\")\n",
        "print(f\" Total Subjects After Augmentation: {num_subjects_after}\")\n",
        "print(f\" Total EEG Epochs Before Augmentation: {total_epochs_before}\")\n",
        "print(f\" Total EEG Epochs After Augmentation: {total_epochs_after}\")\n",
        "print(f\" Increase in Subjects: {num_subjects_after - num_subjects_before} subjects\")\n",
        "print(f\" Increase in Data: {total_epochs_after - total_epochs_before} epochs (+{((total_epochs_after - total_epochs_before) / total_epochs_before) * 100:.2f}%)\\n\")\n",
        "\n",
        "print(\" **EEG Data Shape for 3 Sample Subjects After Augmentation:**\")\n",
        "for subject in sample_subjects:\n",
        "    X = eeg_final[subject].get_data()\n",
        "    print(f\" Subject {subject}: Shape {X.shape} → (Epochs, Channels, Samples)\")\n",
        "\n",
        "print(\"\\n Augmentation successfully increased subjects and data!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTh3FcgC3n9w"
      },
      "outputs": [],
      "source": [
        "#  FINAL FIX: Proper Data Scaling\n",
        "if np.max(np.abs(raw.get_data())) > 1e3:\n",
        "    raw.apply_function(lambda x: x * 1e-6, picks='eeg')  # Scale to Volts (MNE standard)\n",
        "    print(\" Data scaled down to Volts (x1e-6 applied).\")\n",
        "\n",
        "#  Mark as properly converted\n",
        "raw._converted_to_uv = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dDp-P_t3p47"
      },
      "outputs": [],
      "source": [
        "#  Verify Correctness\n",
        "print(f\"Corrected First 5 samples of channel 1: {raw.get_data()[0, :5]}\")\n",
        "print(f\"Max Amplitude After Correction: {np.max(np.abs(raw.get_data())):.2f} V (Should be ~100 µV range)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sctN1AQ9fBGD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def extract_time_domain_features(eeg_data_dict):\n",
        "    \"\"\"\n",
        "    Extracts time-domain features from EEG epoched data.\n",
        "    Features extracted: Mean, Variance, Skewness, Kurtosis for each channel.\n",
        "    \"\"\"\n",
        "    feature_list = []\n",
        "\n",
        "    for subject_id, epochs in eeg_data_dict.items():\n",
        "        print(f\"\\n Extracting Time-Domain Features for {subject_id}...\")\n",
        "\n",
        "        # Get EEG data (Shape: [n_epochs, n_channels, n_samples])\n",
        "        eeg_data = epochs.get_data(picks=\"eeg\")\n",
        "        num_channels = eeg_data.shape[1]  # Ensure we stay within the channel limit\n",
        "\n",
        "        for ch_idx in range(num_channels):\n",
        "            ch_name = epochs.ch_names[ch_idx]  # Get correct channel name\n",
        "            channel_data = eeg_data[:, ch_idx, :]  # Extract all epochs for one channel\n",
        "\n",
        "            # Compute time-domain features per epoch\n",
        "            mean_vals = np.mean(channel_data, axis=1)\n",
        "            var_vals = np.var(channel_data, axis=1)\n",
        "            skew_vals = stats.skew(channel_data, axis=1)\n",
        "            kurt_vals = stats.kurtosis(channel_data, axis=1)\n",
        "\n",
        "            for epoch_idx in range(len(mean_vals)):\n",
        "                feature_list.append({\n",
        "                    \"Subject\": subject_id,\n",
        "                    \"Epoch\": epoch_idx + 1,\n",
        "                    \"Channel\": ch_name,\n",
        "                    \"Mean\": mean_vals[epoch_idx],\n",
        "                    \"Variance\": var_vals[epoch_idx],\n",
        "                    \"Skewness\": skew_vals[epoch_idx],\n",
        "                    \"Kurtosis\": kurt_vals[epoch_idx],\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame and Save\n",
        "    feature_df = pd.DataFrame(feature_list)\n",
        "    feature_df.to_csv(\"/content/time_domain_features.csv\", index=False)\n",
        "    print(\"\\n Time-Domain Features saved!\")\n",
        "\n",
        "    return feature_df\n",
        "\n",
        "#  Run Feature Extraction on Augmented Data\n",
        "time_domain_features = extract_time_domain_features(eeg_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L19vydUqhna7"
      },
      "outputs": [],
      "source": [
        "#  Load extracted features\n",
        "file_path = \"/content/time_domain_features.csv\"\n",
        "\n",
        "try:\n",
        "    time_domain_features = pd.read_csv(file_path)\n",
        "\n",
        "    #  Boxplot Visualization\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.boxplot(data=time_domain_features[['Mean', 'Variance', 'Skewness', 'Kurtosis']])\n",
        "    plt.title(\"Distribution of Time-Domain Features Across All Subjects and Channels\")\n",
        "    plt.xlabel(\"Feature Type\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #  Histogram Plots for Feature Distributions\n",
        "    feature_columns = ['Mean', 'Variance', 'Skewness', 'Kurtosis']\n",
        "\n",
        "    for feature in feature_columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(time_domain_features[feature], bins=50, kde=True)\n",
        "        plt.title(f\"Histogram of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    #  Feature Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(time_domain_features[feature_columns].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The feature extraction CSV file was not found. Please upload the file and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSBlxA1VxN3b"
      },
      "outputs": [],
      "source": [
        "def extract_frequency_domain_features(eeg_data_dict):\n",
        "    \"\"\"\n",
        "    Extracts frequency-domain features from EEG epoched data.\n",
        "    Features extracted: Power Spectral Density (PSD) and Band Power for each EEG band.\n",
        "    \"\"\"\n",
        "    feature_list = []\n",
        "    frequency_bands = {\n",
        "        \"Delta\": (0.5, 4),\n",
        "        \"Theta\": (4, 8),\n",
        "        \"Alpha\": (8, 12),\n",
        "        \"Beta\": (12, 30),\n",
        "        \"Gamma\": (30, 40)\n",
        "    }\n",
        "\n",
        "    for subject_id, epochs in eeg_data_dict.items():\n",
        "        print(f\"\\n Extracting Frequency-Domain Features for {subject_id}...\")\n",
        "\n",
        "        # Get EEG data and sampling frequency\n",
        "        eeg_data = epochs.get_data(picks=\"eeg\")  # Shape: (n_epochs, n_channels, n_samples)\n",
        "        sfreq = epochs.info[\"sfreq\"]\n",
        "        num_channels = eeg_data.shape[1]  # Get actual number of channels\n",
        "        num_samples = eeg_data.shape[2]   # Get actual signal length per epoch\n",
        "\n",
        "        #  Automatically adjust `n_fft`\n",
        "        n_fft = min(256, num_samples)  # If epoch length < 256, set `n_fft = num_samples`\n",
        "\n",
        "        for ch_idx in range(num_channels):\n",
        "            ch_name = epochs.ch_names[ch_idx]\n",
        "            channel_data = eeg_data[:, ch_idx, :]\n",
        "\n",
        "            # Compute Power Spectral Density (PSD) using Welch’s method\n",
        "            psd, freqs = mne.time_frequency.psd_array_welch(\n",
        "                channel_data, sfreq=sfreq, fmin=0.5, fmax=40, n_fft=n_fft, n_per_seg=n_fft\n",
        "            )\n",
        "\n",
        "            # Compute Band Power for each EEG frequency band\n",
        "            band_power = {}\n",
        "            for band, (low, high) in frequency_bands.items():\n",
        "                idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
        "                band_power[band] = np.trapz(psd[:, idx_band], dx=np.diff(freqs).mean(), axis=1)\n",
        "\n",
        "            for epoch_idx in range(psd.shape[0]):\n",
        "                feature_list.append({\n",
        "                    \"Subject\": subject_id,\n",
        "                    \"Epoch\": epoch_idx + 1,\n",
        "                    \"Channel\": ch_name,\n",
        "                    \"PSD_Mean\": np.mean(psd[epoch_idx]),  # Mean PSD over all frequencies\n",
        "                    \"Delta_Power\": band_power[\"Delta\"][epoch_idx],\n",
        "                    \"Theta_Power\": band_power[\"Theta\"][epoch_idx],\n",
        "                    \"Alpha_Power\": band_power[\"Alpha\"][epoch_idx],\n",
        "                    \"Beta_Power\": band_power[\"Beta\"][epoch_idx],\n",
        "                    \"Gamma_Power\": band_power[\"Gamma\"][epoch_idx],\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame and Save\n",
        "    feature_df = pd.DataFrame(feature_list)\n",
        "    feature_df.to_csv(\"/content/frequency_domain_features.csv\", index=False)\n",
        "    print(\"\\n Frequency-Domain Features saved!\")\n",
        "\n",
        "    return feature_df\n",
        "\n",
        "#  Run Feature Extraction on Augmented Data (with Dynamic `n_fft`)\n",
        "frequency_domain_features = extract_frequency_domain_features(eeg_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_HiZEvTBNkZ"
      },
      "outputs": [],
      "source": [
        "#  Load extracted features\n",
        "file_path = \"/content/frequency_domain_features.csv\"\n",
        "\n",
        "try:\n",
        "    frequency_domain_features = pd.read_csv(file_path)\n",
        "\n",
        "    #  Boxplot Visualization\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.boxplot(data=frequency_domain_features[['PSD_Mean', 'Delta_Power', 'Theta_Power', 'Alpha_Power', 'Beta_Power', 'Gamma_Power']])\n",
        "    plt.title(\"Distribution of Frequency-Domain Features Across All Subjects and Channels\")\n",
        "    plt.xlabel(\"Feature Type\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #  Histogram Plots for Feature Distributions\n",
        "    feature_columns = ['PSD_Mean', 'Delta_Power', 'Theta_Power', 'Alpha_Power', 'Beta_Power', 'Gamma_Power']\n",
        "\n",
        "    for feature in feature_columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(frequency_domain_features[feature], bins=50, kde=True)\n",
        "        plt.title(f\"Histogram of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    #  Feature Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(frequency_domain_features[feature_columns].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The feature extraction CSV file was not found. Please upload the file and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tr37A7P_hgu"
      },
      "outputs": [],
      "source": [
        "def extract_wavelet_domain_features(eeg_data_dict):\n",
        "    \"\"\"\n",
        "    Extracts wavelet-domain features from EEG epoched data.\n",
        "    Features extracted: Wavelet Energy, Entropy, and Coefficients.\n",
        "    \"\"\"\n",
        "    feature_list = []\n",
        "    wavelet = \"db4\"  # Daubechies wavelet\n",
        "    decomposition_level = 4  # Number of decomposition levels\n",
        "\n",
        "    for subject_id, epochs in eeg_data_dict.items():\n",
        "        print(f\"\\n Extracting Wavelet-Domain Features for {subject_id}...\")\n",
        "\n",
        "        # Get EEG data\n",
        "        eeg_data = epochs.get_data(picks=\"eeg\")  # Shape: (n_epochs, n_channels, n_samples)\n",
        "        num_channels = eeg_data.shape[1]  # Get actual number of channels\n",
        "\n",
        "        for ch_idx in range(num_channels):\n",
        "            ch_name = epochs.ch_names[ch_idx]\n",
        "            channel_data = eeg_data[:, ch_idx, :]  # Shape: (n_epochs, n_samples)\n",
        "\n",
        "            # Compute wavelet coefficients\n",
        "            wavelet_features = []\n",
        "            for epoch_idx in range(channel_data.shape[0]):\n",
        "                coeffs = pywt.wavedec(channel_data[epoch_idx], wavelet, level=decomposition_level)\n",
        "                coeffs_flattened = np.concatenate(coeffs)  # Flatten coefficients\n",
        "\n",
        "                # Compute Energy (sum of squared coefficients)\n",
        "                energy = np.sum(coeffs_flattened ** 2)\n",
        "\n",
        "                # Compute Entropy (randomness of wavelet coefficients)\n",
        "                prob_distribution = np.abs(coeffs_flattened) / np.sum(np.abs(coeffs_flattened))\n",
        "                wavelet_entropy = entropy(prob_distribution)\n",
        "\n",
        "                wavelet_features.append({\n",
        "                    \"Subject\": subject_id,\n",
        "                    \"Epoch\": epoch_idx + 1,\n",
        "                    \"Channel\": ch_name,\n",
        "                    \"Wavelet_Energy\": energy,\n",
        "                    \"Wavelet_Entropy\": wavelet_entropy,\n",
        "                })\n",
        "\n",
        "            feature_list.extend(wavelet_features)\n",
        "\n",
        "    # Convert to DataFrame and Save\n",
        "    feature_df = pd.DataFrame(feature_list)\n",
        "    feature_df.to_csv(\"/content/wavelet_domain_features.csv\", index=False)\n",
        "    print(\"\\n Wavelet-Domain Features saved!\")\n",
        "\n",
        "    return feature_df\n",
        "\n",
        "#  Run Feature Extraction on Augmented Data\n",
        "wavelet_domain_features = extract_wavelet_domain_features(eeg_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HZShsirxrwm"
      },
      "outputs": [],
      "source": [
        "#  Load extracted features\n",
        "file_path = \"/content/wavelet_domain_features.csv\"\n",
        "\n",
        "try:\n",
        "    wavelet_domain_features = pd.read_csv(file_path)\n",
        "\n",
        "    #  Boxplot Visualization\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.boxplot(data=wavelet_domain_features[['Wavelet_Energy', 'Wavelet_Entropy']])\n",
        "    plt.title(\"Distribution of Wavelet-Domain Features Across All Subjects and Channels\")\n",
        "    plt.xlabel(\"Feature Type\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #  Histogram Plots for Feature Distributions\n",
        "    feature_columns = ['Wavelet_Energy', 'Wavelet_Entropy']\n",
        "\n",
        "    for feature in feature_columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(wavelet_domain_features[feature], bins=50, kde=True)\n",
        "        plt.title(f\"Histogram of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    #  Feature Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(wavelet_domain_features[feature_columns].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The feature extraction CSV file was not found. Please upload the file and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MywEa3iUHOI5"
      },
      "outputs": [],
      "source": [
        "print(os.listdir())  # List all files in the current directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo_llo7HElYM"
      },
      "outputs": [],
      "source": [
        "#  Load all extracted feature datasets\n",
        "time_features = pd.read_csv(\"/content/time_domain_features.csv\")\n",
        "frequency_features = pd.read_csv(\"/content/frequency_domain_features.csv\")\n",
        "wavelet_features = pd.read_csv(\"/content/wavelet_domain_features.csv\")\n",
        "\n",
        "#  Merge all features based on Subject, Epoch, and Channel\n",
        "merged_features = time_features.merge(frequency_features, on=[\"Subject\", \"Epoch\", \"Channel\"], how=\"inner\")\n",
        "merged_features = merged_features.merge(wavelet_features, on=[\"Subject\", \"Epoch\", \"Channel\"], how=\"inner\")\n",
        "\n",
        "#  Save the final combined feature dataset\n",
        "merged_features.to_csv(\"/content/final_combined_features.csv\", index=False)\n",
        "print(f\"\\n Combined Feature Dataset Saved! Shape: {merged_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc47M8XAZuaQ"
      },
      "outputs": [],
      "source": [
        "#  Load Combined Feature Set\n",
        "file_path = \"/content/final_combined_features.csv\"\n",
        "\n",
        "try:\n",
        "    combined_features = pd.read_csv(file_path)\n",
        "\n",
        "    #  Boxplot Visualization\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.boxplot(data=combined_features.drop(columns=[\"Subject\", \"Epoch\", \"Channel\"]))\n",
        "    plt.title(\"Distribution of Combined EEG Features\")\n",
        "    plt.xlabel(\"Feature Type\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #  Feature Correlation Heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(combined_features.drop(columns=[\"Subject\", \"Epoch\", \"Channel\"]).corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap of Combined Features\")\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The final combined feature dataset was not found. Please check if the merging step was completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz9Ea_pgZ5Rz"
      },
      "outputs": [],
      "source": [
        "#  Load Combined Feature Set\n",
        "file_path = \"/content/final_combined_features.csv\"\n",
        "\n",
        "# Load and check basic info\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"\\n Dataset Overview:\")\n",
        "print(df.head())\n",
        "print(\"\\n Missing Values Check:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n Feature Statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQIQ-VMOi8iU"
      },
      "outputs": [],
      "source": [
        "#  Load Full Feature Dataset\n",
        "file_path = \"/content/final_combined_features.csv\"\n",
        "df_full = pd.read_csv(file_path)\n",
        "\n",
        "#  Extract Labels (Modify this based on your classification target)\n",
        "df_full[\"Label\"] = (df_full[\"Alpha_Power\"] > df_full[\"Theta_Power\"]).astype(int)\n",
        "\n",
        "#  Drop Non-Numeric Columns\n",
        "X_full = df_full.drop(columns=[\"Subject\", \"Epoch\", \"Channel\", \"Label\"])\n",
        "y_full = df_full[\"Label\"]\n",
        "\n",
        "#  Feature Selection: Drop Highly Correlated Features\n",
        "corr_matrix = X_full.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "X_full = X_full.drop(columns=high_corr_features)\n",
        "\n",
        "#  Convert Data to Float32 for Memory Optimization\n",
        "X_full = X_full.astype(np.float32)\n",
        "\n",
        "#  Train-Test Split (20% Test Data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Train Optimized Random Forest\n",
        "rf_optimized = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf_optimized.fit(X_train, y_train)\n",
        "y_pred_rf = rf_optimized.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "#  Feature Importance Plot (Top 10 Features)\n",
        "feature_importances = pd.Series(rf_optimized.feature_importances_, index=X_full.columns)\n",
        "top_features = feature_importances.nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_features, y=top_features.index)\n",
        "plt.title(\"Top 10 Important Features (Random Forest)\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.show()\n",
        "\n",
        "#  Train Optimized SVM (Using Important Features Only)\n",
        "top_important_features = top_features.index.tolist()\n",
        "X_train_selected = X_train[top_important_features]\n",
        "X_test_selected = X_test[top_important_features]\n",
        "\n",
        "svm_optimized = SVC(kernel=\"linear\", random_state=42)\n",
        "svm_optimized.fit(X_train_selected, y_train)\n",
        "y_pred_svm = svm_optimized.predict(X_test_selected)\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "#  Print Results\n",
        "print(f\" **Optimized RF Accuracy:** {acc_rf:.4f}\")\n",
        "print(f\" **Optimized SVM Accuracy:** {acc_svm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfgztflAKnJr"
      },
      "outputs": [],
      "source": [
        "#  Load Feature Dataset\n",
        "file_path = \"/content/final_combined_features.csv\"\n",
        "df_cnn = pd.read_csv(file_path)\n",
        "\n",
        "#  Extract Labels (Ensure 1D Integer Labels)\n",
        "df_cnn[\"Label\"] = pd.qcut(df_cnn[\"Alpha_Power\"], q=3, labels=[0, 1, 2]).astype(int)\n",
        "y = df_cnn[\"Label\"].values  # Ensure 1D\n",
        "\n",
        "#  Drop Non-Numeric Columns\n",
        "X = df_cnn.drop(columns=[\"Subject\", \"Epoch\", \"Channel\", \"Label\"])\n",
        "\n",
        "#  Feature Scaling (Normalize for CNN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#  Separate Feature Types (Time, Frequency, Wavelet)\n",
        "time_features = [\"Mean\", \"Variance\", \"Skewness\", \"Kurtosis\"]\n",
        "frequency_features = [\"PSD_Mean\", \"Delta_Power\", \"Theta_Power\", \"Alpha_Power\", \"Beta_Power\", \"Gamma_Power\"]\n",
        "wavelet_features = [\"Wavelet_Energy\", \"Wavelet_Entropy\"]\n",
        "\n",
        "X_time = X_scaled[:, :len(time_features)]\n",
        "X_freq = X_scaled[:, len(time_features): len(time_features) + len(frequency_features)]\n",
        "X_wavelet = X_scaled[:, len(time_features) + len(frequency_features):]\n",
        "\n",
        "#  Merge All Features into One Single Input (Concatenation)\n",
        "X_merged = np.concatenate([X_time, X_freq, X_wavelet], axis=1).astype(np.float32)\n",
        "\n",
        "#  Train-Test Split (80-20 Split with Stratification)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_merged, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "#  Compute Class Weights (AFTER Splitting, with 1D Labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\", classes=np.unique(y_train), y=y_train\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\" Class Weights: {class_weight_dict}\")\n",
        "\n",
        "#  Reshape for CNN (Convert to 3D Tensor)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "print(f\" CNN Input Shape (Merged): {X_train.shape}\")\n",
        "print(f\" Labels Shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y7w5AdXb_Ug"
      },
      "outputs": [],
      "source": [
        "#  Define Residual Block for CNN\n",
        "def residual_block(x, filters):\n",
        "    shortcut = x  # Save input for skip connection\n",
        "\n",
        "    x = Conv1D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv1D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    #  Ensure the shortcut has the same number of filters\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv1D(filters, kernel_size=1, padding=\"same\")(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "#  Define Optimized Multi-Scale CNN\n",
        "def build_multi_scale_cnn(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    #  Feature Extraction with Residual Blocks\n",
        "    x = Conv1D(64, kernel_size=3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = residual_block(x, 128)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = residual_block(x, 256)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    #  Global Average Pooling Instead of Flatten()\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    #  Fully Connected Layers\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)  # Dropout to prevent overfitting\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)  # **Multi-Class Softmax**\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "#  Compute Class Weights Using 1D Labels\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(y_train),  #  Use 1D y_train\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\" Class Weights: {class_weight_dict}\")\n",
        "\n",
        "#  Build Model\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "model = build_multi_scale_cnn(input_shape)\n",
        "\n",
        "#  Compile with Adam & Sparse Categorical Crossentropy\n",
        "optimizer = Adam(learning_rate=0.0005, decay=1e-6)\n",
        "model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
        "\n",
        "#  Callbacks for Stability\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "#  Reshape Input for CNN\n",
        "X_train_cnn = np.expand_dims(X_train, axis=-1)\n",
        "X_test_cnn = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "#  Train Model with Class Weights\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,  #  Use `y_train` directly (1D labels)\n",
        "    epochs=20, batch_size=128,\n",
        "    validation_data=(X_test_cnn, y_test),  #  Use `y_test` directly\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "#  Evaluate Model\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test)  #  No argmax needed\n",
        "print(f\" **CNN Final Test Accuracy:** {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wC9CyB4eONL"
      },
      "outputs": [],
      "source": [
        "#  Plot Training Loss & Accuracy Curves\n",
        "def plot_training_history(history):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss Curve\n",
        "    axs[0].plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "    axs[0].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "    axs[0].set_title(\"Model Loss Over Epochs\")\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Accuracy Curve\n",
        "    axs[1].plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
        "    axs[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "    axs[1].set_title(\"Model Accuracy Over Epochs\")\n",
        "    axs[1].set_xlabel(\"Epochs\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#  Plot Confusion Matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, class_labels):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "#  Compute Model Predictions\n",
        "y_pred = np.argmax(model.predict(X_test_cnn), axis=1)\n",
        "\n",
        "#  Print Classification Report\n",
        "print(\"\\n **CNN Classification Report:**\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "#  Call the Functions for Visualization\n",
        "plot_training_history(history)\n",
        "plot_confusion_matrix(y_test, y_pred, class_labels=[0, 1, 2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooKJOHC6PhgC"
      },
      "outputs": [],
      "source": [
        "#  Ensure SHAP Works with Correct Input Shape\n",
        "X_test_cnn_fixed = np.squeeze(X_test_cnn[:500])  # Ensure correct shape\n",
        "\n",
        "#  Initialize SHAP Explainer\n",
        "explainer = shap.Explainer(model, X_test_cnn_fixed)\n",
        "\n",
        "#  Compute SHAP Values (Fixed Shape)\n",
        "shap_values = explainer(X_test_cnn_fixed)\n",
        "\n",
        "#  Check Shape\n",
        "print(f\" SHAP Values Shape: {np.array(shap_values.values).shape}\")\n",
        "\n",
        "#  Extract Number of Classes Dynamically\n",
        "num_classes = shap_values.values.shape[-1]\n",
        "feature_names = [\"Mean\", \"Variance\", \"Skewness\", \"Kurtosis\",\n",
        "                 \"PSD_Mean\", \"Delta_Power\", \"Theta_Power\",\n",
        "                 \"Alpha_Power\", \"Beta_Power\", \"Gamma_Power\",\n",
        "                 \"Wavelet_Energy\", \"Wavelet_Entropy\"]\n",
        "\n",
        "#  Ensure Correct Shape for Summary Plot\n",
        "if len(shap_values.values.shape) == 3:\n",
        "    for class_idx in range(num_classes):\n",
        "        print(f\"\\n **SHAP Summary for Class {class_idx}**\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        #  Extract SHAP Values for the Class\n",
        "        shap_class_values = shap_values.values[:, :, class_idx]\n",
        "\n",
        "        #  Ensure SHAP Values and Input Data Align\n",
        "        shap.summary_plot(shap_class_values, X_test_cnn_fixed, feature_names=feature_names)\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\" SHAP Value Shape Mismatch! Check Data Input Dimensions.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}